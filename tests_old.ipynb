{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3a70de5-4ddd-4aa5-884e-bfb1a5decfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/learn/audio-course/en/chapter4/fine-tuning\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b020709-932d-449c-a752-4cbee4e38cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ntu-spml/distilhubert\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    model_id, do_normalize=True, return_attention_mask=True\n",
    ")\n",
    "\n",
    "sampling_rate = feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9dc37b-165c-463c-ae21-678a737e4850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/audio/main/tutorials/audio_io_tutorial.html\n",
    "test_file_path = 'Rebetika/Bellou/Bellou_sil_remov_1.wav'\n",
    "waveform, sample_rate = torchaudio.load(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b83c7c-6b56-40dd-b82b-469d9245bf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 672333])\n"
     ]
    }
   ],
   "source": [
    "print(waveform.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d848f73-d4dd-4161-b3ad-1aa8eb122ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/audio/stable/tutorials/audio_resampling_tutorial.html\n",
    "resample_rate = 16000\n",
    "resampler = T.Resample(sample_rate, resample_rate, dtype=waveform.dtype)\n",
    "resampled_waveform = resampler(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4760d30d-c3dd-4073-a766-232834a78424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 243931])\n"
     ]
    }
   ],
   "source": [
    "print(resampled_waveform.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1be435c4-ba7b-4f88-928a-6a4c2e827f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and resample audio file to desired characteristics\n",
    "def load_and_resample_audio_file(file_path, resample_rate=16000):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    resampler = T.Resample(sample_rate, resample_rate, dtype=waveform.dtype)\n",
    "    return resampler(waveform)\n",
    "# end load_and_resample_audio_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c95201b6-2a7a-4693-9298-e97fc16a9d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs keys: ['input_values', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# feature extractor will handle audio files, e.g. normalize and truncate to max length\n",
    "inputs = feature_extractor( resampled_waveform , sampling_rate=resample_rate)\n",
    "print(f\"inputs keys: {list(inputs.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85d031bc-55e8-4299-8611-25d6b10dc20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 243931)\n",
      "-4.1715507e-09\n",
      "0.99999756\n",
      "[array([1], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "print(inputs['input_values'][0].shape)\n",
    "print( np.mean(inputs['input_values']) )\n",
    "print( np.std(inputs['input_values']) )\n",
    "print(inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f437d1a-581a-4f1b-bbe6-924a2f41fb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kazantzidis_old', 'Ninou', 'Bellou', 'Kazantzidis', 'Tsaousakis']\n"
     ]
    }
   ],
   "source": [
    "class_names = os.listdir('Rebetika')\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a1fffe8-4cfa-4762-9b83-ce34318153ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'kazantzidis_old', '1': 'Ninou', '2': 'Bellou', '3': 'Kazantzidis', '4': 'Tsaousakis'}\n",
      "{'kazantzidis_old': '0', 'Ninou': '1', 'Bellou': '2', 'Kazantzidis': '3', 'Tsaousakis': '4'}\n"
     ]
    }
   ],
   "source": [
    "id2label = {\n",
    "    str(i): c for i, c in enumerate(class_names)\n",
    "}\n",
    "label2id = {v: k for k,v in id2label.items()}\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ad9f9e0-3d37-4beb-9484-003037d84e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [{'array': tensor([[ 0.0168,  0.0335,  0.0374,  ..., -0.0438, -0.0348, -0.0249]]), 'class': 0}, {'array': tensor([[ 0.0198,  0.0196, -0.0042,  ...,  0.0197,  0.0208,  0.0199]]), 'class': 0}, {'array': tensor([[-0.0136, -0.0238, -0.0243,  ...,  0.0178,  0.0216,  0.0196]]), 'class': 0}, {'array': tensor([[0.0138, 0.0228, 0.0220,  ..., 0.0412, 0.0447, 0.0423]]), 'class': 0}, {'array': tensor([[0.0138, 0.0222, 0.0203,  ..., 0.0201, 0.0220, 0.0070]]), 'class': 0}, {'array': tensor([[0.0148, 0.0234, 0.0210,  ..., 0.0197, 0.0193, 0.0215]]), 'class': 0}, {'array': tensor([[0.0146, 0.0261, 0.0258,  ..., 0.0209, 0.0228, 0.0085]]), 'class': 0}, {'array': tensor([[ 0.0172, -0.0026, -0.0174,  ...,  0.0334,  0.0423,  0.0241]]), 'class': 0}, {'array': tensor([[0.0140, 0.0242, 0.0244,  ..., 0.0148, 0.0052, 0.0149]]), 'class': 0}, {'array': tensor([[ 4.6234e-05, -2.0508e-05, -1.5664e-04,  ...,  1.3930e-03,\n",
      "          1.2100e-03,  6.1904e-05]]), 'class': 1}, {'array': tensor([[0.0137, 0.0217, 0.0194,  ..., 0.0183, 0.0229, 0.0114]]), 'class': 1}, {'array': tensor([[5.6719e-05, 1.9558e-04, 2.0463e-04,  ..., 5.3480e-03, 3.0559e-03,\n",
      "         8.1352e-04]]), 'class': 1}, {'array': tensor([[ 0.0189,  0.0069, -0.0208,  ...,  0.0007,  0.0006,  0.0003]]), 'class': 1}, {'array': tensor([[-0.0008, -0.0014, -0.0011,  ...,  0.0153,  0.0219,  0.0159]]), 'class': 1}, {'array': tensor([[-0.0035, -0.0046, -0.0031,  ...,  0.0464,  0.1252,  0.0515]]), 'class': 1}, {'array': tensor([[0.0156, 0.0264, 0.0203,  ..., 0.0006, 0.0005, 0.0007]]), 'class': 1}, {'array': tensor([[-4.3450e-05, -9.1516e-05, -1.1944e-04,  ...,  1.9643e-02,\n",
      "          1.9803e-02,  1.9994e-02]]), 'class': 1}, {'array': tensor([[0.0140, 0.0220, 0.0171,  ..., 0.0220, 0.0212, 0.0188]]), 'class': 1}, {'array': tensor([[-0.0054, -0.0073, -0.0041,  ...,  0.0211,  0.0219,  0.0182]]), 'class': 1}, {'array': tensor([[0.0137, 0.0216, 0.0194,  ..., 0.0074, 0.0160, 0.0197]]), 'class': 2}, {'array': tensor([[-0.0002, -0.0002,  0.0001,  ...,  0.0009,  0.0019,  0.0007]]), 'class': 2}, {'array': tensor([[-0.0005, -0.0011, -0.0010,  ..., -0.0006, -0.0007, -0.0010]]), 'class': 2}, {'array': tensor([[ 0.0164,  0.0139, -0.0199,  ...,  0.0157,  0.0198,  0.0193]]), 'class': 2}, {'array': tensor([[0.0139, 0.0233, 0.0204,  ..., 0.0111, 0.0202, 0.0160]]), 'class': 2}, {'array': tensor([[-6.2918e-05, -1.0027e-04, -1.6861e-04,  ..., -1.8657e-03,\n",
      "          2.1605e-03,  3.7051e-03]]), 'class': 2}, {'array': tensor([[0.0137, 0.0222, 0.0202,  ..., 0.0173, 0.0185, 0.0216]]), 'class': 2}, {'array': tensor([[0.0140, 0.0218, 0.0180,  ..., 0.0231, 0.0226, 0.0219]]), 'class': 2}, {'array': tensor([[ 0.0145,  0.0225,  0.0178,  ..., -0.0271, -0.0196, -0.0027]]), 'class': 2}, {'array': tensor([[0.0003, 0.0005, 0.0005,  ..., 0.0163, 0.0213, 0.0124]]), 'class': 2}, {'array': tensor([[0.0144, 0.0237, 0.0196,  ..., 0.0188, 0.0242, 0.0114]]), 'class': 2}, {'array': tensor([[0.0148, 0.0241, 0.0166,  ..., 0.0204, 0.0221, 0.0130]]), 'class': 2}, {'array': tensor([[ 4.6670e-05,  1.2513e-05, -1.5506e-04,  ...,  2.2257e-04,\n",
      "          7.8042e-04,  8.9864e-04]]), 'class': 2}, {'array': tensor([[0.0039, 0.0062, 0.0050,  ..., 0.0193, 0.0219, 0.0104]]), 'class': 2}, {'array': tensor([[ 0.0003,  0.0008,  0.0010,  ..., -0.0267, -0.0262, -0.0037]]), 'class': 2}, {'array': tensor([[-2.1590e-05,  1.1848e-05,  6.2706e-05,  ...,  1.5342e-02,\n",
      "          2.0352e-02,  1.6410e-02]]), 'class': 2}, {'array': tensor([[0.0140, 0.0222, 0.0198,  ..., 0.0151, 0.0223, 0.0121]]), 'class': 2}, {'array': tensor([[0.0234, 0.0515, 0.0247,  ..., 0.0113, 0.0173, 0.0187]]), 'class': 2}, {'array': tensor([[4.5720e-05, 1.2620e-04, 1.3881e-04,  ..., 1.8166e-02, 1.9438e-02,\n",
      "         1.9371e-02]]), 'class': 2}, {'array': tensor([[1.3894e-02, 2.3834e-02, 2.2853e-02,  ..., 2.7203e-05, 3.2298e-05,\n",
      "         2.6646e-05]]), 'class': 2}, {'array': tensor([[ 0.0142,  0.0220,  0.0189,  ..., -0.0011,  0.0056,  0.0034]]), 'class': 2}, {'array': tensor([[0.0141, 0.0235, 0.0206,  ..., 0.0163, 0.0233, 0.0104]]), 'class': 2}, {'array': tensor([[0.0233, 0.0839, 0.1304,  ..., 0.0218, 0.0240, 0.0092]]), 'class': 3}, {'array': tensor([[0.0140, 0.0250, 0.0268,  ..., 0.0235, 0.0234, 0.0185]]), 'class': 3}, {'array': tensor([[0.0140, 0.0231, 0.0198,  ..., 0.0211, 0.0203, 0.0219]]), 'class': 3}, {'array': tensor([[0.0145, 0.0220, 0.0153,  ..., 0.0196, 0.0254, 0.0123]]), 'class': 3}, {'array': tensor([[0.0141, 0.0221, 0.0186,  ..., 0.0227, 0.0246, 0.0101]]), 'class': 3}, {'array': tensor([[0.0168, 0.0270, 0.0218,  ..., 0.0218, 0.0236, 0.0082]]), 'class': 3}, {'array': tensor([[0.0144, 0.0206, 0.0117,  ..., 0.0160, 0.0210, 0.0243]]), 'class': 3}, {'array': tensor([[ 0.0256,  0.0267, -0.0294,  ...,  0.0300,  0.0292,  0.0200]]), 'class': 3}, {'array': tensor([[0.0149, 0.0278, 0.0308,  ..., 0.0201, 0.0216, 0.0164]]), 'class': 3}, {'array': tensor([[0.0142, 0.0250, 0.0240,  ..., 0.0191, 0.0215, 0.0195]]), 'class': 3}, {'array': tensor([[0.0180, 0.0429, 0.0608,  ..., 0.0255, 0.0229, 0.0232]]), 'class': 3}, {'array': tensor([[0.0142, 0.0249, 0.0256,  ..., 0.0173, 0.0196, 0.0220]]), 'class': 3}, {'array': tensor([[0.0172, 0.0364, 0.0415,  ..., 0.0142, 0.0194, 0.0236]]), 'class': 3}, {'array': tensor([[ 0.0174,  0.0333,  0.0319,  ..., -0.0100, -0.0014,  0.0196]]), 'class': 3}, {'array': tensor([[0.0144, 0.0253, 0.0254,  ..., 0.0179, 0.0184, 0.0208]]), 'class': 3}, {'array': tensor([[ 0.0165,  0.0220, -0.0004,  ...,  0.0258,  0.0243,  0.0227]]), 'class': 3}, {'array': tensor([[ 1.5933e-02,  2.9421e-02,  2.0157e-02,  ..., -5.8852e-05,\n",
      "          3.3256e-05,  4.6234e-05]]), 'class': 3}, {'array': tensor([[0.0016, 0.0031, 0.0020,  ..., 0.0188, 0.0222, 0.0089]]), 'class': 4}, {'array': tensor([[-0.0003,  0.0003,  0.0009,  ...,  0.0121,  0.0103,  0.0090]]), 'class': 4}, {'array': tensor([[0.0149, 0.0240, 0.0092,  ..., 0.0229, 0.0237, 0.0120]]), 'class': 4}, {'array': tensor([[ 0.0157,  0.0023, -0.0216,  ...,  0.0008,  0.0001, -0.0002]]), 'class': 4}, {'array': tensor([[0.0150, 0.0226, 0.0171,  ..., 0.0169, 0.0201, 0.0158]]), 'class': 4}, {'array': tensor([[ 0.0141,  0.0253,  0.0256,  ..., -0.0035, -0.0034, -0.0035]]), 'class': 4}, {'array': tensor([[ 0.0143,  0.0243,  0.0239,  ..., -0.0037, -0.0029, -0.0007]]), 'class': 4}, {'array': tensor([[0.0134, 0.0200, 0.0162,  ..., 0.0089, 0.0205, 0.0192]]), 'class': 4}, {'array': tensor([[0.0159, 0.0254, 0.0066,  ..., 0.0222, 0.0238, 0.0116]]), 'class': 4}, {'array': tensor([[-0.0007, -0.0012, -0.0014,  ..., -0.0097, -0.0230, -0.0205]]), 'class': 4}, {'array': tensor([[0.0139, 0.0231, 0.0209,  ..., 0.0230, 0.0239, 0.0147]]), 'class': 4}, {'array': tensor([[-1.0748e-06,  2.2814e-06, -3.9464e-05,  ...,  1.8011e-04,\n",
      "          6.5260e-05,  6.3481e-05]]), 'class': 4}, {'array': tensor([[ 0.0137,  0.0226,  0.0223,  ..., -0.0115, -0.0135,  0.0145]]), 'class': 4}, {'array': tensor([[ 0.0139,  0.0223,  0.0208,  ..., -0.0010,  0.0008,  0.0004]]), 'class': 4}, {'array': tensor([[ 0.0141,  0.0241,  0.0237,  ..., -0.0002, -0.0002, -0.0004]]), 'class': 4}, {'array': tensor([[ 0.0150,  0.0284,  0.0299,  ...,  0.0013,  0.0003, -0.0001]]), 'class': 4}, {'array': tensor([[0.0142, 0.0251, 0.0258,  ..., 0.0120, 0.0211, 0.0162]]), 'class': 4}, {'array': tensor([[ 0.0146,  0.0250,  0.0242,  ...,  0.0019,  0.0014, -0.0002]]), 'class': 4}, {'array': tensor([[0.0140, 0.0235, 0.0229,  ..., 0.0216, 0.0209, 0.0206]]), 'class': 4}, {'array': tensor([[0.0138, 0.0225, 0.0212,  ..., 0.0174, 0.0190, 0.0217]]), 'class': 4}, {'array': tensor([[-0.0008, -0.0012, -0.0011,  ...,  0.0205,  0.0225,  0.0111]]), 'class': 4}, {'array': tensor([[ 0.0153,  0.0285,  0.0290,  ..., -0.0008, -0.0014, -0.0010]]), 'class': 4}, {'array': tensor([[0.0138, 0.0214, 0.0181,  ..., 0.0185, 0.0229, 0.0090]]), 'class': 4}, {'array': tensor([[-0.0001, -0.0002, -0.0001,  ...,  0.0026,  0.0043,  0.0029]]), 'class': 4}, {'array': tensor([[0.0163, 0.0329, 0.0383,  ..., 0.0210, 0.0224, 0.0141]]), 'class': 4}, {'array': tensor([[0.0169, 0.0343, 0.0104,  ..., 0.0052, 0.0075, 0.0045]]), 'class': 4}, {'array': tensor([[0.0148, 0.0241, 0.0129,  ..., 0.0012, 0.0013, 0.0014]]), 'class': 4}, {'array': tensor([[ 8.9327e-06,  1.3388e-06, -5.1382e-05,  ...,  4.9700e-03,\n",
      "          5.0414e-03,  5.5124e-03]]), 'class': 4}, {'array': tensor([[0.0235, 0.0376, 0.0318,  ..., 0.3203, 0.3354, 0.3117]]), 'class': 4}, {'array': tensor([[0.0143, 0.0242, 0.0236,  ..., 0.0015, 0.0162, 0.0130]]), 'class': 4}, {'array': tensor([[ 4.6249e-05, -3.3662e-05, -7.6112e-05,  ..., -6.5219e-03,\n",
      "         -9.2842e-03, -7.6221e-03]]), 'class': 4}, {'array': tensor([[ 0.0143,  0.0262,  0.0259,  ..., -0.0002, -0.0004, -0.0008]]), 'class': 4}, {'array': tensor([[0.0151, 0.0242, 0.0103,  ..., 0.0033, 0.0068, 0.0202]]), 'class': 4}, {'array': tensor([[0.0138, 0.0156, 0.0019,  ..., 0.0103, 0.0175, 0.0179]]), 'class': 4}], 'test': [{'array': tensor([[0.0138, 0.0229, 0.0216,  ..., 0.0154, 0.0150, 0.0230]]), 'class_idx': 0}, {'array': tensor([[0.0137, 0.0183, 0.0106,  ..., 0.0225, 0.0233, 0.0161]]), 'class_idx': 0}, {'array': tensor([[-0.0015, -0.0031, -0.0031,  ...,  0.0250,  0.0235,  0.0232]]), 'class_idx': 1}, {'array': tensor([[-3.2422e-05,  5.4663e-05, -4.1726e-05,  ..., -6.0404e-02,\n",
      "         -6.5350e-02, -7.3410e-02]]), 'class_idx': 1}, {'array': tensor([[ 0.0142,  0.0253,  0.0216,  ..., -0.0022, -0.0025, -0.0014]]), 'class_idx': 2}, {'array': tensor([[ 0.0159,  0.0274,  0.0209,  ..., -0.0176,  0.0197,  0.0253]]), 'class_idx': 2}, {'array': tensor([[0.0138, 0.0224, 0.0207,  ..., 0.0196, 0.0222, 0.0095]]), 'class_idx': 2}, {'array': tensor([[ 0.0137,  0.0224,  0.0212,  ..., -0.0037, -0.0097, -0.0069]]), 'class_idx': 2}, {'array': tensor([[ 0.0164,  0.0202, -0.0060,  ...,  0.0199,  0.0227,  0.0160]]), 'class_idx': 2}, {'array': tensor([[ 0.0143,  0.0191,  0.0095,  ..., -0.0190,  0.0082,  0.0130]]), 'class_idx': 3}, {'array': tensor([[0.0139, 0.0231, 0.0170,  ..., 0.0720, 0.0770, 0.0434]]), 'class_idx': 3}, {'array': tensor([[ 0.0169,  0.0323,  0.0307,  ...,  0.0018, -0.0001, -0.0014]]), 'class_idx': 3}, {'array': tensor([[0.0143, 0.0237, 0.0211,  ..., 0.0240, 0.0258, 0.0134]]), 'class_idx': 3}, {'array': tensor([[ 0.0140,  0.0230,  0.0199,  ..., -0.0085,  0.0028,  0.0056]]), 'class_idx': 4}, {'array': tensor([[0.0179, 0.0248, 0.0149,  ..., 0.0244, 0.0263, 0.0174]]), 'class_idx': 4}, {'array': tensor([[ 0.0010,  0.0018,  0.0020,  ..., -0.0734, -0.0578, -0.0478]]), 'class_idx': 4}, {'array': tensor([[0.0140, 0.0205, 0.0111,  ..., 0.0189, 0.0218, 0.0125]]), 'class_idx': 4}, {'array': tensor([[ 0.0155,  0.0340,  0.0363,  ..., -0.0022, -0.0009,  0.0003]]), 'class_idx': 4}, {'array': tensor([[-4.8258e-05, -1.1732e-04, -1.1959e-04,  ..., -8.5291e-03,\n",
      "         -1.3803e-02, -1.6130e-02]]), 'class_idx': 4}, {'array': tensor([[0.0150, 0.0237, 0.0193,  ..., 0.0211, 0.0218, 0.0185]]), 'class_idx': 4}, {'array': tensor([[ 0.0143,  0.0249,  0.0252,  ...,  0.0003, -0.0002, -0.0001]]), 'class_idx': 4}]}\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "rebetika = {\n",
    "    'train': [],\n",
    "    'test': []\n",
    "}\n",
    "\n",
    "for c_i, c in enumerate( class_names ):\n",
    "    class_files = os.listdir('Rebetika/' + c)\n",
    "    for f_i, file_name in enumerate( class_files ):\n",
    "        if not file_name.startswith('.'):\n",
    "            if f_i <= len(class_files)*train_test_ratio:\n",
    "                rebetika['train'].append(\n",
    "                    {\n",
    "                        'array' : load_and_resample_audio_file('Rebetika/' + c + os.sep + file_name, resample_rate=16000),\n",
    "                        'class': int( label2id[ c ] )\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                rebetika['test'].append(\n",
    "                    {\n",
    "                        'array' : load_and_resample_audio_file('Rebetika/' + c + os.sep + file_name, resample_rate=16000),\n",
    "                        'class_idx': int( label2id[ c ] )\n",
    "                    }\n",
    "                )\n",
    "            # end if\n",
    "        # end if\n",
    "    # end for class_files\n",
    "# end for class_names\n",
    "\n",
    "print(rebetika)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "320c453e-3d84-41cb-ab36-260893b9d9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67a48b5b1c44b8cbe9e036cbf41d9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a57b80dfbc4e888d4977354b1ba928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5969dbc36dc448de9d00082f77fdbf6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c654b67cb8d401ca064e5458e26ffcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('audiofolder', data_dir='Rebetika')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "781563d6-db97-4388-92bf-974cbf4f931e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/media/maximos/9C33-6BBD/python/audio_singer_classification/Rebetika/train/Tsaousakis/Tsaousakis_sil_remov_1.wav', 'array': array([ 2.03247070e-02,  2.17895508e-02,  2.29187012e-02, ...,\n",
      "       -1.83105469e-04, -6.10351562e-05, -3.05175781e-05]), 'sampling_rate': 44100}, 'label': 3}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['Bellou', 'Kazantzidis', 'Ninou', 'Tsaousakis', 'kazantzidis_old'], id=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset['train'][50])\n",
    "dataset['train'].features['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "984ff987-2862-414c-b6b9-ce1e43e3d78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at ntu-spml/distilhubert and are newly initialized: ['classifier.bias', 'classifier.weight', 'encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(id2label)\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=num_labels,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fbfb5ad-7604-4529-a32f-e31762cc6be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_id.split(\"/\")[-1]\n",
    "batch_size = 8\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 10\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-rebetika_voice\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cd21946-54b9-4590-99a6-98791307d678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d30c98d8-2237-43cc-a23a-d8556d786b37",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You should supply an instance of `transformers.BatchFeature` or list of `transformers.BatchFeature` to this method that includes input_values, but you provided []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      4\u001b[0m     model,\n\u001b[1;32m      5\u001b[0m     training_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/torchaudio/lib/python3.12/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/torchaudio/lib/python3.12/site-packages/transformers/trainer.py:1836\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1833\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1835\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1836\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1839\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/torchaudio/lib/python3.12/site-packages/accelerate/data_loader.py:451\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/torchaudio/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda/envs/torchaudio/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda/envs/torchaudio/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/torchaudio/lib/python3.12/site-packages/transformers/trainer_utils.py:772\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[\u001b[38;5;28mdict\u001b[39m]):\n\u001b[1;32m    771\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_columns(feature) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m--> 772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/torchaudio/lib/python3.12/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda/envs/torchaudio/lib/python3.12/site-packages/transformers/data/data_collator.py:59\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# To avoid errors when using Feature extractors\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecation_warnings\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Save the state of the warning, then disable it\u001b[39;00m\n\u001b[1;32m     62\u001b[0m warning_state \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda/envs/torchaudio/lib/python3.12/site-packages/transformers/feature_extraction_sequence_utils.py:132\u001b[0m, in \u001b[0;36mSequenceFeatureExtractor.pad\u001b[0;34m(self, processed_features, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_values`, has be passed for padding\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_features:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an instance of `transformers.BatchFeature` or list of `transformers.BatchFeature`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to this method that includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(processed_features\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m     )\n\u001b[1;32m    138\u001b[0m required_input \u001b[38;5;241m=\u001b[39m processed_features[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    139\u001b[0m return_attention_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    140\u001b[0m     return_attention_mask \u001b[38;5;28;01mif\u001b[39;00m return_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_attention_mask\n\u001b[1;32m    141\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: You should supply an instance of `transformers.BatchFeature` or list of `transformers.BatchFeature` to this method that includes input_values, but you provided []"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=rebetika[\"train\"],\n",
    "    eval_dataset=rebetika[\"test\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
